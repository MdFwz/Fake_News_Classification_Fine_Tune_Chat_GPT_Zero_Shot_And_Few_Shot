{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eade14a0-d525-4ac7-94be-8150e1b860c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (4.3.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
      "     -------------------------------------- 243.9/243.9 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (4.25.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.6/190.6 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\moham\\anaconda3\\lib\\site-packages (from wandb) (63.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.5)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.34.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ab65d9-1041-44ec-a211-3b6f61e8e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\moham\\anaconda3\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\moham\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\moham\\anaconda3\\lib\\site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f480021c-84e6-49f9-8e48-1ce74ed47f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai 1.3.3\n"
     ]
    }
   ],
   "source": [
    "!openai --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0ad74e6-14c9-45f2-a6ca-6eaa39613d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\moham\\anaconda3\\lib\\site-packages (0.28.1)\n",
      "Collecting openai\n",
      "  Using cached openai-1.3.3-py3-none-any.whl (220 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting typing-extensions<5,>=4.5\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.5.1-py3-none-any.whl (381 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Collecting httpcore\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\moham\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.14)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.14.3\n",
      "  Downloading pydantic_core-2.14.3-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 12.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.5)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, h11, distro, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "Successfully installed annotated-types-0.6.0 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3 pydantic-2.5.1 pydantic-core-2.14.3 typing-extensions-4.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea74b1b-e992-4db1-ac9b-541605961f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c74cf68-0352-4324-8345-69b67ca7831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eafc77ec-fdf6-4711-bcd4-586eb47292f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import openai\n",
    "print(openai.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc66da83-4e5d-4929-887d-be1fcca90928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m ada --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n",
      "Executing cmd = openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2\n",
      "CompletedProcess(args='openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m curie --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2', returncode=1)\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "import time\n",
    "\n",
    "\n",
    "class ModelRun:\n",
    "    \"\"\" Client to make requests to the OpenAI given hyper-parameters\"\"\"\n",
    "\n",
    "    def __init__(self, base_model: str, n_epochs: int, batch_size: int, learning_rate_multiplier: float,\n",
    "                 openai_apikey: str, train_file, valid_file, test_file, compute_classification_metrics=True):\n",
    "        self.base_model = base_model\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_multiplier = learning_rate_multiplier\n",
    "        self.compute_classification_metrics = compute_classification_metrics\n",
    "        self.api_key = openai_apikey\n",
    "        self.n_classes = 6\n",
    "        self.train_file = train_file\n",
    "        self.valid_file = valid_file\n",
    "        self.test_file = test_file\n",
    "\n",
    "        # preprocess files for fine-tuning consumption\n",
    "        train_ds = LiarDataset(file_path=self.train_file, name=\"liar_train\")\n",
    "        train_ds.process()\n",
    "        train_ds.write()\n",
    "\n",
    "        valid_ds = LiarDataset(file_path=self.valid_file, name=\"liar_valid\")\n",
    "        valid_ds.process()\n",
    "        valid_ds.write()\n",
    "\n",
    "        test_ds = LiarDataset(file_path=self.test_file, name=\"liar_test\")\n",
    "        test_ds.process()\n",
    "        test_ds.write()\n",
    "\n",
    "        train_df = train_ds.get()\n",
    "        valid_df = valid_ds.get()\n",
    "        dev_df = pd.concat([train_df, valid_df])\n",
    "        dev_df.to_json(f\"dev.jsonl\", orient='records', lines=True)\n",
    "\n",
    "    def request_finetune(self):\n",
    "        cmd = f\"\"\"openai api fine_tunes.create -t liar_train.jsonl -v liar_valid.jsonl -m {self.base_model} --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size {self.batch_size} --n_epochs {self.n_epochs} --learning_rate_multiplier {self.learning_rate_multiplier}\n",
    "        \"\"\"\n",
    "        print(f\"Executing cmd = {cmd}\")\n",
    "        return self.subprocess_run(cmd)\n",
    "\n",
    "    def request_finetune_dev(self):\n",
    "        cmd = f\"\"\"openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m {self.base_model} --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size {self.batch_size} --n_epochs {self.n_epochs} --learning_rate_multiplier {self.learning_rate_multiplier}\"\"\"\n",
    "        print(f\"Executing cmd = {cmd}\")\n",
    "        return self.subprocess_run(cmd)\n",
    "\n",
    "    def subprocess_run(self, cmd: bytes) -> str:\n",
    "        try:\n",
    "            return subprocess.run(cmd, input=\" \\n \\n\".encode(), shell=True, env={\"OPENAI_API_KEY\": self.api_key})\n",
    "        except subprocess.CalledProcessError:\n",
    "            raise ValueError(f\"Failed to execute the command: {cmd}\")\n",
    "\n",
    "\n",
    "class LiarDataset:\n",
    "    prompt_end = \"\\n\\n###\\n\\n\"\n",
    "    completion_start = \" \"\n",
    "    completion_end = \"###\"\n",
    "\n",
    "    def __init__(self, file_path, name):\n",
    "        self.data: pd.DataFrame = pd.read_csv(file_path,\n",
    "                                              index_col=False,\n",
    "                                              delimiter=\"\\t\",\n",
    "                                              header=None,\n",
    "                                              names=[\"id\", \"label\", \"statement\", \"subject\",\n",
    "                                                     \"speaker\", \"speaker_job_title\", \"state_info\",\n",
    "                                                     \"party_affiliation\",\n",
    "                                                     \"barely_true_c\", \"half_true_c\", \"false_c\", \"mostly_true_c\",\n",
    "                                                     \"pantsonfire_c\", \"context\"])\n",
    "        self.name = name\n",
    "\n",
    "    def process(self, include_meta=False):\n",
    "\n",
    "        def process_label(x):\n",
    "            return f\"{LiarDataset.completion_start}{x}{LiarDataset.completion_end}\"\n",
    "\n",
    "        def process_prompt(x):\n",
    "            return f\"{x}{LiarDataset.prompt_end}\"\n",
    "\n",
    "        def process_prompt_with_meta(x):\n",
    "            \"\"\"Use space to concat statement subject context fields.\n",
    "            \"\"\"\n",
    "            result = f\"{x['statement']} {x['subject']} {x['context']}{LiarDataset.prompt_end}\"\n",
    "            return result\n",
    "\n",
    "        self.data[\"completion\"] = self.data[\"label\"].map(lambda x: process_label(x))\n",
    "\n",
    "        if include_meta:\n",
    "            self.data[\"prompt\"] = self.data.apply(lambda x: process_prompt_with_meta(x), axis=1)\n",
    "        else:\n",
    "            self.data[\"prompt\"] = self.data[\"statement\"].map(lambda x: process_prompt(x))\n",
    "\n",
    "        self.data = self.data.drop_duplicates()\n",
    "        self.data = self.data[[\"prompt\", \"completion\"]]\n",
    "\n",
    "    def get(self):\n",
    "        return self.data\n",
    "\n",
    "    def write(self):\n",
    "        self.data.to_json(f\"{self.name}.jsonl\", orient='records', lines=True)\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. hyper-parameter tune using train and valid datasets    #\n",
    "    learning_rate_multipliers = [0.02, 0.04, 0.08, 0.16, 0.2, 0.22]\n",
    "    n_epochs = [2, 3, 4, 5]\n",
    "    trials = 5\n",
    "\n",
    "    for lrm in learning_rate_multipliers:\n",
    "        for ne in n_epochs:\n",
    "            for i in range(trials):\n",
    "                mr = ModelRun(base_model=\"ada\",\n",
    "                              n_epochs=ne,\n",
    "                              batch_size=256,\n",
    "                              learning_rate_multiplier=lrm,\n",
    "                              openai_apikey=\"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\",\n",
    "                              train_file=\"liar_dataset/train.tsv\",\n",
    "                              valid_file=\"liar_dataset/valid.tsv\",\n",
    "                              test_file=\"liar_dataset/test.tsv\")\n",
    "                result = mr.request_finetune()\n",
    "                time.sleep(5)\n",
    "\n",
    "                print(result)\n",
    "                print(\"---------------------------------------------------------------------------\")\n",
    "                \n",
    "\"\"\"\n",
    "    # get all runs from the CMD and publish to W&B\n",
    "    # openai api fine_tunes.list\n",
    "    # openai wandb sync\n",
    "\n",
    "    # Train it using train + valid dataset and compute metrics on the test set (5) times\n",
    "    # ada and curie method.\n",
    "models = [\"ada\", \"curie\"]\n",
    "for model in models:\n",
    "    for i in range(trials):\n",
    "            # use best hyper-parameters to get test accuracy\n",
    "        mr = ModelRun(base_model=model,\n",
    "                          n_epochs=5,\n",
    "                          batch_size=256,\n",
    "                          learning_rate_multiplier=0.2,\n",
    "                          openai_apikey=\"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\",\n",
    "                          train_file=\"liar_dataset/fine_tune_train.tsv\",\n",
    "                          valid_file=\"liar_dataset/fine_tune_valid.tsv\",\n",
    "                          test_file=\"liar_dataset/fine_tune_test.tsv\")\n",
    "        result = mr.request_finetune_dev()\n",
    "        time.sleep(5)\n",
    "\n",
    "        print(result)\n",
    "        print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "    # get all runs from the CMD and publish to W&B\n",
    "    # openai api fine_tunes.list\n",
    "    # openai wandb sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7a0aec38-448c-4042-bfab-f04deabb2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: openai api [-h]\n",
      "                  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}\n",
      "                  ...\n",
      "openai api: error: argument {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}: invalid choice: 'fine_tunes.create' (choose from 'chat.completions.create', 'images.generate', 'images.edit', 'images.create_variation', 'audio.transcriptions.create', 'audio.translations.create', 'files.create', 'files.retrieve', 'files.delete', 'files.list', 'models.list', 'models.retrieve', 'models.delete', 'completions.create')\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m \"ada\" --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bce8b3-4d51-4f17-a67c-5601830da1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e7a820-cd21-495a-9634-922d857f68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e685a4a4-f6cc-465e-8962-8c8584d87997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.tsv',sep='\\t', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cf2c2aff-7dd1-4bdf-bc5a-febabfe56005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'undefined'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion(statement:str):    \n",
    "    p = statement\n",
    "    \n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"ft:gpt-3.5-turbo-1106:personal::8NeFAJeJ\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": p\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    " \n",
    "    \n",
    "    response_state = response.choices[0].finish_reason\n",
    "    label = \"\"\n",
    "    evidence = \"\"\n",
    "    if response_state != \"stop\":\n",
    "        return \"invalid_response\"\n",
    "    else:\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        if \"label\" in response_text.lower():\n",
    "            label = response_text.lower().split(\"label:\")[1].split(\"Label_End\")[0].lower().strip()\n",
    "       \n",
    "\n",
    "        if \"pants-fire\" in label:\n",
    "            label = \"pants-fire\"\n",
    "        elif \"false\" in label:\n",
    "            label= \"false\"\n",
    "        elif \"mostly-true\" in label:\n",
    "            label= \"mostly-true\"\n",
    "        elif \"barely-true\" in label:\n",
    "            label= \"barely-true\"\n",
    "        elif \"half-true\" in label:\n",
    "            label= \"half-true\"\n",
    "        elif \"true\" in label:\n",
    "            label= \"true\" \n",
    "        else:\n",
    "            label= \"undefined\"\n",
    "            \n",
    "        return label\n",
    "    \n",
    "get_completion(\"Medicaid spending declined by 1.9 percent in 2012, the second such decline in 47 years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae8b5a16-e817-4a92-bcac-a16af153014b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mostly-true'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def askAI(statement):\n",
    "    p = statement+\"\\n\\n###\\n\\n\"\n",
    "    client = OpenAI()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ada fine tuned\n",
    "    response = client.completions.create(\n",
    "      model=\"ada:ft-personal:liar-2023-11-17-21-28-20\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=100,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    # Davinci fine tuned\n",
    "    \n",
    "    response = client.completions.create(\n",
    "      model=\"ft:davinci-002:personal::8MIb3GOl\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    response = client.completions.create(\n",
    "      model=\"ft:babbage-002:personal::8Nbmn0jG\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \"\"\"\n",
    "    # This code is for v1 of the openai package: pypi.org/project/openai\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"ft:gpt-3.5-turbo-1106:personal::8NeFAJeJ\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": p\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    response_state = response.choices[0].finish_reason\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    label = \"\"\n",
    "    evidence = \"\"\n",
    "    response_text = response_text.lower()\n",
    "\n",
    "    # Define the keywords in the order of priority\n",
    "    keywords = [\"pants-fire\", \"false\", \"mostly-true\", \"barely-true\", \"half-true\", \"true\"]\n",
    "\n",
    "    # Initialize variables to store the first occurrence and the corresponding label\n",
    "    first_occurrence = len(response_text)\n",
    "    label = \"undefined\"\n",
    "\n",
    "    # Check for each keyword in the response text\n",
    "    for keyword in keywords:\n",
    "        occurrence = response_text.find(keyword)\n",
    "        if occurrence != -1 and occurrence < first_occurrence:\n",
    "            first_occurrence = occurrence\n",
    "            label = keyword\n",
    "\n",
    "# Print and return the label\n",
    "    return label\n",
    "\n",
    "    \n",
    "   \n",
    "  \n",
    "askAI(\"Medicaid spending declined by 1.9 percent in 2012, the second such decline in 47 years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ed2fafe9-303a-4d90-8dea-dfdaabac7804",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13384\\1473399823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0maskAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Medicaid spending declined by 1.9 percent in 2012, the second such decline in 47 years.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13384\\1473399823.py\u001b[0m in \u001b[0;36maskAI\u001b[1;34m(statement)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Extract the content of the last message from the bot, which contains the response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mresponse_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse_messages\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'role'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'system'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13384\\1473399823.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Extract the content of the last message from the bot, which contains the response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mresponse_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse_messages\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'role'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'system'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def askAI(statement):\n",
    "    p = statement+\"\\n\\n###\\n\\n\"\n",
    "    client = OpenAI()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ada fine tuned\n",
    "    response = client.completions.create(\n",
    "      model=\"ada:ft-personal:liar-2023-11-17-21-28-20\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=100,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    # Davinci fine tuned\n",
    "    \n",
    "    response = client.completions.create(\n",
    "      model=\"ft:davinci-002:personal::8MIb3GOl\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    response = client.completions.create(\n",
    "      model=\"ft:babbage-002:personal::8Nbmn0jG\",\n",
    "      prompt=p,\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    # This code is for v1 of the openai package: pypi.org/project/openai\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"ft:gpt-3.5-turbo-1106:personal::8NeFAJeJ\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": p\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    # The response object structure has changed, this is the new way to access the text.\n",
    "    response_state = response['choices'][0]['message']['content'] if 'content' in response['choices'][0]['message'] else \"\"\n",
    "    \n",
    "    # Assuming that 'content' is the key where the text of the response is stored.\n",
    "    response_text = response_state.lower()\n",
    "    \n",
    "    print(response_text)\n",
    "    \n",
    "    label = \"\"\n",
    "    evidence = \"\"\n",
    "    \n",
    "    # Define the keywords in the order of priority\n",
    "    keywords = [\"pants-fire\", \"false\", \"mostly-true\", \"barely-true\", \"half-true\", \"true\"]\n",
    "\n",
    "    # Initialize variables to store the first occurrence and the corresponding label\n",
    "    first_occurrence = len(response_text)\n",
    "    label = \"undefined\"\n",
    "\n",
    "    # Check for each keyword in the response text\n",
    "    for keyword in keywords:\n",
    "        occurrence = response_text.find(keyword)\n",
    "        if occurrence != -1 and occurrence < first_occurrence:\n",
    "            first_occurrence = occurrence\n",
    "            label = keyword\n",
    "\n",
    "    # Print and return the label\n",
    "    return label\n",
    "\n",
    "askAI(\"Medicaid spending declined by 1.9 percent in 2012, the second such decline in 47 years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a79518bd-3af2-4a12-9b28-7bc222fd6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:46<00:00,  4.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store results\n",
    "idx = []\n",
    "labels = []\n",
    "statements = []\n",
    "\n",
    "# Loop through the DataFrame's rows\n",
    "for i in tqdm(range(1167, df.shape[0])):\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    \n",
    "    # Try to get a label for the statement with retries\n",
    "    while attempt < 5 and not success:\n",
    "        try:\n",
    "            st = df.iloc[i][2]\n",
    "            label = askAI(st)\n",
    "            # If the API call was successful, append the results\n",
    "            idx.append(i)\n",
    "            labels.append(label)\n",
    "            statements.append(st)\n",
    "            success = True\n",
    "\n",
    "            # Wait before the next API call\n",
    "            time.sleep(4)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(f\"Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            attempt += 1\n",
    "\n",
    "    # If after several attempts it doesn't work, break or handle accordingly\n",
    "    if not success:\n",
    "        a = pd.DataFrame({\"idx\": idx, \"label\": labels, \"Statement\":statements})\n",
    "        a.to_csv(f\"fine_tune_babbage_result_to_{i}.csv\")\n",
    "        print(\"Maximum retry attempts reached, exiting.\")\n",
    "        break\n",
    "    else:\n",
    "        a = pd.DataFrame({\"idx\": idx, \"label\": labels, \"Statement\":statements})\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "203084c2-7cd9-41c6-8d78-d1a158095438",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(f\"fine_tune_babbage_result_to_{i}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6f348-b6c7-41f2-8c9a-1b00e59edb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df.iloc[, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9bed6bd0-306c-4882-b9eb-c495f10f39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: openai api [-h]\n",
      "                  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}\n",
      "                  ...\n",
      "openai api: error: argument {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}: invalid choice: 'fine_tunes.create' (choose from 'chat.completions.create', 'images.generate', 'images.edit', 'images.create_variation', 'audio.transcriptions.create', 'audio.translations.create', 'files.create', 'files.retrieve', 'files.delete', 'files.list', 'models.list', 'models.retrieve', 'models.delete', 'completions.create')\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t dev.jsonl -v liar_test.jsonl -m \"ada\" --compute_classification_metrics --classification_n_classes 6 --suffix liar --batch_size 256 --n_epochs 5 --learning_rate_multiplier 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a462456b-0c1e-4edb-a9af-b0411410334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: openai api [-h]\n",
      "                  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}\n",
      "                  ...\n",
      "\n",
      "positional arguments:\n",
      "  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}\n",
      "                        All API subcommands\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!openai api --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e9d4f5ae-646d-422a-a9ce-9a6b0ff0495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1957379636937648\n"
     ]
    }
   ],
   "source": [
    "aa = pd.read_csv(\"fine_tune_babbage_result_to_1167.csv\", index_col=0)\n",
    "b = pd.read_csv(\"fine_tune_babbage_result_to_1266.csv\", index_col=0)\n",
    "final = pd.concat([b,aa])\n",
    "y_actual = df.iloc[:, 1]\n",
    "y_predicted  = final.iloc[:,1]\n",
    "print(f\"Accuracy: {accuracy_score(y_actual, y_predicted)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
